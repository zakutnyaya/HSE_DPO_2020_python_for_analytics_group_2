{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python для анализа данных\n",
    "\n",
    "# Spark / PySpark \n",
    "\n",
    "#### автор: Валентин Бирюков\n",
    "\n",
    "\n",
    "Spark является все более популярной кластерной вычислительной системой на основе Apache Hadoop, которая предлагает большую потенциальную ценность благодаря своей скорости и простоте использования. Мы рассмотрим его здесь, уделив особое внимание интерфейсу Python для Spark: PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка\n",
    "-------------\n",
    "\n",
    "Для работы нам потребуется собствено сам Spark который можно скачать и установить с официального сайта http://spark.apache.org/downloads.html\n",
    "\n",
    "Так же для его успешного функционирования потребуется Java8/11. И вот тут могут возникнуть сложности, поскольку сейчас последняя и поддерживаемая верся - Java11, но самостоятельная настройка может вызывать затруднение при совместимости пакетов, такие как ошибка вида:\n",
    "\n",
    "`Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe. : java.lang.IllegalArgumentException: Unsupported class file major version 55` \n",
    "\n",
    "в таком случае самый простой вариант запустить данный блокнот используя **Google Colab**. \n",
    "\n",
    "\n",
    "\n",
    "*Замечание 1:\n",
    "При локальном запуске и запуске в virtualenv мы должны указать Spark использовать текущую версию Python, иначе она будет использовать системную версию Python по умолчанию. Вставьте это в свой код: `os.environ['PYSPARK_PYTHON'] = sys.executable`.*\n",
    "\n",
    "*Замечание 2:\n",
    "Spark имеет веб-интерфейс, который показывает запущенные задачи, выполняющиеся процессы и различную статистику. Запуская локально, это может наблюдать в интерфесе http://localhost:4040/.*\n",
    "\n",
    "*Замечание 3:\n",
    "Запуская же ноутбук в **colab** чтобы получить такую ссылку раскомментируйте ячейку ниже и запустите ее. По этой ссылке будет доступен аналог локального хоста только для облачного блокнота. По этой ссылке доступ будет только у вас, залогиненных в учетной записи google. Для других же пользователей эта ссылка будет выдавать 403 ошибку - ошибку доступа к ресурсу.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab.output import eval_js\n",
    "# print(eval_js(\"google.colab.kernel.proxyPort(4040)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поставим сам модуль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращения к pyspark\n",
    "---------------\n",
    "\n",
    "Чтобы вызвать Spark из Python, нам нужно использовать интерфейс PySpark. Например, его можно вызвать интерактивной оболочкой из вашей домашней директории Spark.:\n",
    "\n",
    "    ./bin/pyspark\n",
    "\n",
    "Как оболочка iPython Spark:\n",
    "\n",
    "    IPYTHON=1 ./bin/pyspark\n",
    "\n",
    "Или как пусковая установка для скриптов:\n",
    "\n",
    "    ./bin/pyspark --master local\n",
    "\n",
    "Ниже мы рассмотрим, как использовать API PySpark внутри скриптов Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Spark's home directory (here it's: ~/spark-1.6.0) should be set as an environment variable.\n",
    "# (Of course setting an env. variable doesn't need to be done from Python; any method will do.)\n",
    "# os.environ['SPARK_HOME'] = os.path.join(os.path.expanduser('~'), 'spark-1.6.0')\n",
    "\n",
    "# Add Spark's Python interface (PySpark) to PYTHONPATH.\n",
    "# (Again: this doesn't need to be done from Python.)\n",
    "# sys.path.append(os.path.join(os.environ.get('SPARK_HOME'), 'python'))\n",
    "\n",
    "# This can be useful for running in virtualenvs:\n",
    "# os.environ['PYSPARK_PYTHON'] = '/home/nico/virtualenv/bin/python'\n",
    "\n",
    "# OK, now we can import PySpark\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внутри нашей *рабочей программки* соединение с Spark представлено экземпляром `SparkContext`. Для локального запуска Spark вы можете просто создать его с помощью:\n",
    "\n",
    "    sc = SparkContext('local', 'mySparkApp')\n",
    "\n",
    "Кроме того, вы можете использовать экземпляр `SparkConf` для управления различными свойствами конфигурации Spark, что мы и будем рассматривать ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark.app.name=spark_tutorial\\nspark.cores.max=4\\nspark.executor.memory=1g\\nspark.master=local\\nspark.submit.deployMode=client\\nspark.ui.showConsoleProgress=true'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# укажем что будем мы запускать все это локально\n",
    "conf.setMaster('local')\n",
    "conf.setAppName('spark_tutorial') # некоторый alias нашего \"приложения\"\n",
    "# SparkConf имеет методы 'set', 'setAll' и 'setIfMissing' которые могут быть использованы\n",
    "# для уточнения конфигурации нашего \"кластера\" - той части которую мы хотим заиспользовать\n",
    "# например - задействовать 4 ядра и 1Gb оперативы\n",
    "conf.setIfMissing(\"spark.cores.max\", \"4\")\n",
    "conf.set(\"spark.executor.memory\", \"1g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Другой вариан, задать все это разом:\n",
    "conf.setAll([('spark.cores.max', '4'), ((\"spark.executor.memory\", \"1g\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# И теперь запустим spark с такой конфигурацией\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# остановить же это можно с помощью следующей команды в ручном режиме:\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как работает Spark, очень-очень вкратце\n",
    "-------------------------\n",
    "\n",
    "Spark использует *диспетчер кластеров* (например, собственный автономный менеджер Spark, YARN или Mesos) и несколько *рабочих узлов*. Менеджер задач (ака master/main) пытается получить *исполнителей* (ака slaves/secondary) на рабочих узлах, которые выполняют вычисления и хранят данные на основе кода и задач, которые им отправляются.\n",
    "\n",
    "\n",
    "Основная абстракция Spark - это так называемый *Resilient Distributed Dataset (RDD)*. Spark может создавать RDD из любого источника хранения, поддерживаемого Hadoop. RDD содержит промежуточные результаты вычислений и хранится в ОЗУ или на диске на рабочих узлах. В случае сбоя узла, RDD может быть восстановлен. Многие процессы могут выполняться параллельно благодаря распределенной природе RDD, а конвейерная обработка и отложенное выполнение предотвращают необходимость сохранения промежуточных результатов для следующего шага. Важно отметить, что Spark поддерживает извлечение наборов данных в кластерный *кэш в памяти* для быстрого доступа.\n",
    "\n",
    "Операции RDD можно разделить на 2 группы: *преобразования* (transform) и *действия* (actions). Преобразования (например, `map`) RDD всегда приводят к новым RDD, а действия (например, `reduce`) возвращают значения, которые являются результатом операций над RDD, обратно в программу драйвера.\n",
    "\n",
    "Звучит сложно, но из кода попробуем понять это более наглядно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD и распределенные данные\n",
    "\n",
    "Сейчас, когда мы запускаем все это дело локально на одной машине - они в реалиях не очень то распределенные, они лежат на физическом одном диске. Однако даже в этом случае запускаясь локально Spark будет оркестрировать всем, как будто у него маленький кластер. Настолько маленький, что ровно из одного вашего компьютера =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 4\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:50)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$4$$anonfun$visitMethodInsn$7.apply(ClosureCleaner.scala:845)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$4$$anonfun$visitMethodInsn$7.apply(ClosureCleaner.scala:828)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$4.visitMethodInsn(ClosureCleaner.scala:828)\n\tat org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n\tat org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:272)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:271)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:271)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2326)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2100)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b03b6bd206cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of partitions: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 'glom' lists all elements within each partition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:50)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$4$$anonfun$visitMethodInsn$7.apply(ClosureCleaner.scala:845)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$4$$anonfun$visitMethodInsn$7.apply(ClosureCleaner.scala:828)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$4.visitMethodInsn(ClosureCleaner.scala:828)\n\tat org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n\tat org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:272)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:271)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:271)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2326)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2100)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "# 'parallelize' создает RDD путем распределения данных по кластеру\n",
    "rdd = sc.parallelize(range(14), numSlices=4)\n",
    "# по сути создаем список из 14 элементов которой храним распределенно, на 4 \"файлах\"\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "# 'glom' перечисляет все элементы в каждом разделе\n",
    "print(rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark - ленивый\n",
    "Несмотря на любые промежуточные преобразования, Spark запускается только после выполнения *действия* на RDD. Это связано с тем, что он пытается выполнить умную конвейеризацию операций, чтобы не сохранять промежуточные результаты.\n",
    "\n",
    "Этакий знакомый аналог `map` в питоне, который по факту еще ничего не применяет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169]\n"
     ]
    }
   ],
   "source": [
    "rddSquared = rdd.map(lambda x: x ** 2)\n",
    "\n",
    "print(rddSquared.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Альтернативный вариант, с созданием функции:\n",
    "def squared(x):\n",
    "    return x ** 2\n",
    "rddSquared = rdd.map(squared)\n",
    "print(rddSquared.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных обоих случаях только `collect` инициировал работу с данными, остальные же созданные преобразования откладывались как \"состояния\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим другие популярные преобразования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразования\n",
    "# -----------------------\n",
    "\n",
    "func = lambda x: -x\n",
    "rdd.map(func)\n",
    "rdd.flatMap(func) # почти как map, только результат будет распакован\n",
    "rdd.filter(func)\n",
    "rdd.sortBy(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# действия\n",
    "# ---------------\n",
    "\n",
    "rdd.reduce(lambda x, y: x + y)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обоих этих случаях операции по сути никуда не применились, можно сказать что мы выстроили процесс по которому будут выполняться узлы, однако каждый из них вел в \"никуда\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "0\n",
      "[0, 1, 2, 3, 4]\n",
      "[13, 12, 11]\n",
      "[13, 12, 11, 10, 9, 8, 7]\n"
     ]
    }
   ],
   "source": [
    "# Действия, возвращающие полученные в данные\n",
    "print(rdd.collect())                    # вернуть все эллементы\n",
    "print(rdd.first())                      # вернуть первый элемент\n",
    "print(rdd.take(5))                      # вернуть первые N элементов\n",
    "print(rdd.top(3))                       # Вернуть первые N элементов упорядоченные по убыванию\n",
    "print(rdd.takeOrdered(7, lambda x: -x)) # Вернуть N эллементов, отсортированных согласно какой то \"функции\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение: Решето Эратосфена.\n",
    "\n",
    "Напишите алгоритм просеивания простых чисел оперирую pyspark\n",
    "\n",
    "Подсказка: все не так то просто, последовательные фильтры надо явно заставлять выполнять"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD может использовать кеши\n",
    "Spark позволяет пользователю контролировать, какие данные и как кэшируются. Правильное кэширование RDD может быть чрезвычайно полезным! Всякий раз, когда у вас есть RDD, который будет использоваться повторно несколько раз, вам следует рассмотреть возможность его кэширования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NUM_SAMPLES = int(1e6)\n",
    "rddBig = sc.parallelize(np.random.random(NUM_SAMPLES))\n",
    "\n",
    "# нет кэширования: будет пересчитываться каждый раз, когда мы проходим цикл\n",
    "rddBigTrans = rddBig.map(lambda x: (x ** 2 - 0.1) ** 0.5)\n",
    "print(rddBigTrans.getStorageLevel())\n",
    "for threshold in (0.2, 0.4, 0.6, 0.8):\n",
    "    %timeit -n 1 -r 1 rddBigTrans.filter(lambda x: x >= threshold).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# мы кешируем этот промежуточный результат, потому что он будет неоднократно вызываться\n",
    "rddBigTrans_c = rddBig.map(lambda x: (x ** 2 - 0.1) ** 0.5).cache()\n",
    "print(rddBigTrans_c.getStorageLevel())\n",
    "for threshold in (0.2, 0.4, 0.6, 0.8):\n",
    "    %timeit -n 1 -r 1 rddBigTrans_c.filter(lambda x: x >= threshold).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-abe5d729a470>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mNUM_SAMPLES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrddBig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_SAMPLES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# используем unpersist для удаления из кэша\n",
    "print(rddBigTrans_c.unpersist().getStorageLevel())\n",
    "# для еще более детального управления кэшированием используйте функцию «persist» \n",
    "from pyspark import storagelevel\n",
    "print(rddBigTrans.persist(storagelevel.StorageLevel.MEMORY_AND_DISK).getStorageLevel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark: key-value хранилище\n",
    "Так называемые PairRDD - это RDD, в которых хранятся пары ключ-значение. В Spark используется множество специальных операций, таких как объединение по ключу, группирование по ключу и т. д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PairRDD автоматически создаются всякий раз, когда мы представляем список кортежей ключ-значение\n",
    "# Здесь мы трансформируем rddA и создаем ключ на основе четных / нечетных флагов.\n",
    "rddP1 = rdd.map(lambda x: (x % 2 == 0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Более понятный вариант для этого:\n",
    "rddP1 = rdd.keyBy(lambda x: x % 2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Другой способ создать PairRDD - это заархивировать два RDD (предполагается, что RDD одинаковой длины)\n",
    "print(\"Zipped: {}\".format(rdd.zip(rdd).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Доступ к ключам и значениям\n",
    "print(\"Keys: {}\".format(rddP1.keys().collect()))\n",
    "print(\"Values: {}\".format(rddP1.values().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Другой вариант обращения к ключам-значением - через кортеж; x[0] - key, x[1] - value\n",
    "print(rddP1.map(lambda x: (x[0], x[1] ** 2)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лучше: mapValues / flatMapValues, который работает только со значениями и сохраняет ключи на месте\n",
    "print(rddP1.mapValues(lambda x: x ** 2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Мы также можем вернуться от PairRDD к обычному RDD, просто опустив ключ\n",
    "print(rddP1.map(lambda x: x[1] ** 2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возможны различные агрегации по ключу, такие как reduceByKey, combineByKey и foldByKey\n",
    "# пример с reduceByKey:\n",
    "print(\"Sum per key: {}\".format(rddP1.reduceByKey(lambda x, y: x + y).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кроме того, некоторые общие операции доступны в форме «ByKey», например:\n",
    "rddP1.sortByKey()\n",
    "rddP1.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Группировка и соединение по ключу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Существуют различные возможные способы объединения двух RDD по ключу:\n",
    "rddP2 = sc.parallelize(range(0, 28, 2)).map(lambda x: (x % 2 == 0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join / cross join в случае наложения ключей\n",
    "print(\"Join: {}\".format(rddP1.join(rddP2).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left/right outer join\n",
    "rddP1.leftOuterJoin(rddP2)\n",
    "rddP1.rightOuterJoin(rddP2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для всех ключей в rddP1 или rddP2 cogroup возвращает итерируемые значения\n",
    "print(\"Cogroup: {}\".format(rddP1.cogroup(rddP2).collect()))\n",
    "# Группируем вместе более двух RDD по ключу можно с помощью groupWith\n",
    "rddP1.groupWith(rddP2, rddP2)\n",
    "\n",
    "# с groupByKey мы создаем новый RDD, который сохраняет те же ключи на том же узле, где это возможно\n",
    "print(\"After groupByKey: {}\".format(rddP1.groupByKey().glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark: работа напряму с созданием фреймов RDD из текстовых файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Вариант для colab, локально можно поискать другие удобные файлы\n",
    "from pyspark import SparkFiles\n",
    "sc.addFile(os.path.join('/content/sample_data', 'README.md'))\n",
    "rddT = sc.textFile(SparkFiles.get('README.md'))\n",
    "print(rddT.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs простые статистические аггрегаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdd.stats())\n",
    "print(rdd.count())\n",
    "print(rdd.sum())\n",
    "print(rdd.mean())\n",
    "print(rdd.stdev(), rdd.sampleStdev())\n",
    "print(rdd.variance(), rdd.sampleVariance())\n",
    "print(rdd.min(), rdd.max())\n",
    "print(rdd.histogram(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs преобразования множеств"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddB = sc.parallelize(range(0, 26, 2))\n",
    "print(rdd.union(rddB).collect()) # or: rdd + rddB\n",
    "print(rdd.union(rddB).distinct().collect())\n",
    "print(rdd.intersection(rddB).collect())\n",
    "print(rdd.subtract(rddB).collect())\n",
    "print(rdd.cartesian(rddB).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark поддержка общих переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Общая переменная копируется на каждую машину только один раз, эффективным образом.\n",
    "# Это очень удобно, когда каждый узел использует данные в нем, и особенно, если данные\n",
    "# большие и в противном случае будут отправлены по сети несколько раз.\n",
    "broadcastVar = sc.broadcast({'CA': 'California', 'NL': 'Netherlands'})\n",
    "print(broadcastVar.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Аккумулятор\" является общей переменной, которая живет на главном узле,\n",
    "# который каждая операция может просматривать.\n",
    "accu = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'foreach' просто применяет функцию к каждому элементу RDD, ничего не возвращая\n",
    "rdd.foreach(lambda x: accu.add(x))\n",
    "print(accu.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Популярные баги:\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не кэшировать промежуточные результаты, которые  используются позже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Not so great:\")\n",
    "rddBigTrans = rddBig.map(lambda x: (x ** 2 - 0.1) ** 0.5)\n",
    "for threshold in (0.2, 0.4, 0.6, 0.8):\n",
    "    %timeit -n 1 -r 1 rddBigTrans.filter(lambda x: x >= threshold).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Better:\")\n",
    "rddBigTrans_c = rddBig.map(lambda x: (x ** 2 - 0.1) ** 0.5).cache()\n",
    "for threshold in (0.2, 0.4, 0.6, 0.8):\n",
    "    %timeit -n 1 -r 1 rddBigTrans_c.filter(lambda x: x >= threshold).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не учитывать, когда и как данные передаются через кластер\n",
    "Имейте в виду, что Spark является распределенной вычислительной средой и что следует избегать передачи данных по сети внутри кластера (пропускная способность сети в ~100 раз дороже пропускной способности памяти)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupByKey запускает случайное воспроизведение, поэтому по сети копируется много данных\n",
    "sumPerKey = rddP1.groupByKey().mapValues(lambda x: sum(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Лучше: reduceByKey уменьшает ту передачу локально перед \"перетасовкой\"\n",
    "sumPerKey = rddP1.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не работать с соответствующим количеством разделов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Недостаточное количество разделов приводит к плохому параллелизму в кластере.\n",
    "\n",
    "Это также оказывает нагрузку на память для определенных операций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# С другой стороны, предположим, что RDD распределен по 1000 разделам,\n",
    "# но мы работаем только над небольшим подмножеством данных в RDD, например:\n",
    "rddF = rdd.filter(lambda x: x < 0.1).map(lambda x: x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Затем мы эффективно создаем много пустых задач и используем объединение или перераспределение.\n",
    "# было бы полезно создать RDD с меньшим количеством разделов\n",
    "rddF = rdd.filter(lambda x: x < 3).coalesce(10).map(lambda x: x ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используя преобразование с высокими накладными расходами на элемент, лучше использовать mapPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Например, просто подключение к базе и отключени от нее уже требует расходов\n",
    "def db_operation(x):\n",
    "    # тут мы подключилис\n",
    "    # Поделали что-то с элементом\n",
    "    # завершаем действие, отключаемся от базы\n",
    "    pass\n",
    "\n",
    "# Особенно, если вы повторите это для каждого элемента:\n",
    "rdd.map(db_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Лучше: делайте это на уровне раздела, а не на уровне элемента.\n",
    "def vectorized_db_operation(x):\n",
    "    # тут мы подключились\n",
    "    # Поделали что-то с элементом\n",
    "    # завершаем действие, отключаемся от базы\n",
    "    pass\n",
    "\n",
    "# в таком случае мы будем обрабатывать даныне целиком пачками, они все будут вычитывать в память за раз\n",
    "result = rdd.mapPartitions(vectorized_db_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отправка большого количества данных вместе с вызовом функции для каждого элемента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigData = np.random.random(int(1e6)) #наши \"большие данные\", мы ж все же на одной машинке работаем\n",
    "\n",
    "def myFunc(x):\n",
    "    return x * np.random.choice(bigData)\n",
    "\n",
    "# и тогда наш массив будет отправляться в каждую партицию, то есть просто гоняться по сети в холостую\n",
    "rdd.map(myFunc)\n",
    "\n",
    "# Лучше: сделать большие данные доступными только для чтения, чтобы они эффективно копировались по сети\n",
    "bigDataBC = sc.broadcast(bigData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В боевых задачах\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поучим что-нибудь скайлерном"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим полусинтетический пример. Создадим какую нибудь выборку данных, из которых мы захотим решить задачу регересси.\n",
    "\n",
    "Вот только решение ее - будем делать распределенно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split, ShuffleSplit\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn import pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "N = 10000   # number of data points\n",
    "D = 100     # number of dimensions\n",
    "\n",
    "X, y = make_regression(\n",
    "    n_samples=N,\n",
    "    n_features=D,\n",
    "    n_informative=int(D*0.1),\n",
    "    n_targets=1,\n",
    "    bias=-6.,\n",
    "    noise=50.,\n",
    "    random_state=42\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# раскидаем данные случаным образом по партициям\n",
    "samples = sc.parallelize(ShuffleSplit(y_train.size, n_iter=8))\n",
    "reg_model = pipeline.Pipeline([(\"scaler\", StandardScaler()), (\"ridge\", Ridge())])\n",
    "# это кусочек обработки данных для обучения - перегоним переменные в нормальное распределение нормировкой,\n",
    "# и потом будем запусать на них решение задачи гребневой регрессии\n",
    "\n",
    "# обучим модель на каждой пачке и примеyим к выборке\n",
    "mean_rsq = samples.map(\n",
    "    lambda (index, _): reg_model.fit(X[index], y[index]).score(X_test, y_test)\n",
    ").mean()\n",
    "print(mean_rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "получили такой самопальный вариант нескольких моделей, которые как-то голосуют за данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение: \"не боевая\" работа \"с боевыми\" данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам потребуется датасет с ценами на жилье https://www.kaggle.com/camnugent/california-housing-prices\n",
    "попробуем пообрабатывать его не привычным пандасом, а с использованием спарка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "считаем датасет и приведем его в человеческий вид"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "# поможет нам собрать из строчек более привычный пандасовский вариант\n",
    "# здесь нам лучше избавиться пока от заголовка в файле,\n",
    "# зато сделать данные более удобными назначива постолбцовое хранение\n",
    "rdd = sc.textFile('/content/sample_data/california_housing_train.csv')\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2 \n",
    "\n",
    "теперь проведем все колонки в типизированный вид, там же пока строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 3\n",
    "\n",
    "Добавим новых признаков:\n",
    "    * комнат на домовладельцев\n",
    "    * жителей на домовладение\n",
    "    * доля спальных комнат относительно всех"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стахостический градиентный спуск своими руками, как бонус"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent using scikit-learn (from: https://gist.github.com/MLnick/4707012)\n",
    "Each partition is a mini-batch for the SGD, uses average weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm\n",
    "from sklearn.base import copy\n",
    "\n",
    "N = 10000   # Number of data points\n",
    "D = 10      # Numer of dimensions\n",
    "ITERATIONS = 5\n",
    "np.random.seed(seed=42)\n",
    "\n",
    "def generate_data(N):\n",
    "    return [[[1] if np.random.rand() < 0.5 else [0], np.random.randn(D)]\n",
    "            for _ in range(N)]\n",
    "\n",
    "def train(iterator, sgd):\n",
    "    for x in iterator:\n",
    "        sgd.partial_fit(x[1], x[0], classes=np.array([0, 1]))\n",
    "    yield sgd\n",
    "\n",
    "def merge(left, right):\n",
    "    new = copy.deepcopy(left)\n",
    "    new.coef_ += right.coef_\n",
    "    new.intercept_ += right.intercept_\n",
    "    return new\n",
    "\n",
    "def avg_model(sgd, slices):\n",
    "    sgd.coef_ /= slices\n",
    "    sgd.intercept_ /= slices\n",
    "    return sgd\n",
    "\n",
    "slices = 4\n",
    "data = generate_data(N)\n",
    "print(len(data))\n",
    "\n",
    "# init stochastic gradient descent\n",
    "sgd = lm.SGDClassifier(loss='log')\n",
    "# training\n",
    "for ii in range(ITERATIONS):\n",
    "    sgd = sc.parallelize(data, numSlices=slices) \\\n",
    "            .mapPartitions(lambda x: train(x, sgd)) \\\n",
    "            .reduce(lambda x, y: merge(x, y))\n",
    "    # averaging weight vector => iterative parameter mixtures\n",
    "    sgd = avg_model(sgd, slices)\n",
    "    print(\"Iteration %d:\" % (ii + 1))\n",
    "    print(\"Model: \")\n",
    "    print(sgd.coef_)\n",
    "    print(sgd.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark universe\n",
    "------------------\n",
    "\n",
    "Other interesting tools for Spark:\n",
    "\n",
    "- Spark SQL: http://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "- MLlib, Spark's machine learning library: http://spark.apache.org/docs/latest/mllib-guide.html\n",
    "- Spark Streaming, for streaming data applications: http://spark.apache.org/docs/latest/streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "\n",
    "Spark documentation: https://spark.apache.org/docs/latest/index.html\n",
    "\n",
    "Spark programming guide: http://spark.apache.org/docs/latest/programming-guide.html\n",
    "\n",
    "PySpark documentation: https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "### Books\n",
    "\n",
    "Learning Spark: http://shop.oreilly.com/product/0636920028512.do\n",
    "\n",
    "(preview: https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/)\n",
    "\n",
    "### Talks (recommended to watch them in this order)\n",
    "\n",
    "Parallel programming with Spark: https://www.youtube.com/watch?v=7k4yDKBYOcw\n",
    "\n",
    "Advanced Spark features: https://www.youtube.com/watch?v=w0Tisli7zn4\n",
    "\n",
    "PySpark: Python API for Spark: https://www.youtube.com/watch?v=xc7Lc8RA8wE\n",
    "\n",
    "Understanding Spark performance: https://www.youtube.com/watch?v=NXp3oJHNM7E\n",
    "\n",
    "A deeper understanding of Spark's internals: https://www.youtube.com/watch?v=dmL0N3qfSc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
